{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auteurs : CHÉRUEL Valentine, DE LA PURIFICATION Loane, LEY Clément"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "UuQIqRgJtkXn",
    "outputId": "e45f9369-2c5a-4dd0-ec1e-e21dd9c20869"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWn4PPpptkX0"
   },
   "source": [
    "# Pré-processing de textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "COx4XiS3tkX1"
   },
   "source": [
    "Cette [page](https://keras.io/preprocessing/text/) détaille les méthodes de pré-processing de texte avec Keras et présente notamment la classe Tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ia2UwTetkX2"
   },
   "source": [
    "> Completer le code ci-dessous pour créer un analyseur lexical (tokenizer) avec keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zllsV--itkX3"
   },
   "outputs": [],
   "source": [
    "samples = [\n",
    "    \"Je suis un étudiant en ingénierie à Nantes !\",\n",
    "    \"J'adore les mathématiques appliquées et la finance.\",\n",
    "    \"Les modèles de machine learning sont passionnants.\",\n",
    "    \"Je mixe régulièrement dans des clubs et festivals.\",\n",
    "    \"La programmation en Python est essentielle pour mes projets.\",\n",
    "    \"Je produis ma propre musique depuis plusieurs années.\",\n",
    "    \"Les marchés financiers sont un domaine complexe mais fascinant.\",\n",
    "    \"J'ai travaillé sur un projet de prédiction des marchés électriques.\",\n",
    "    \"L'importance des variables dans les forêts aléatoires m'intéresse beaucoup.\",\n",
    "    \"Je suis passionné par l'analyse de données et les algorithmes.\",\n",
    "    \"Je participe à des compétitions de data science.\",\n",
    "    \"Mon objectif est de poursuivre un master en finance quantitative.\",\n",
    "    \"J'organise des événements et gère des équipes en parallèle de mes études.\",\n",
    "    \"Je joue du piano, de la guitare et de la batterie.\",\n",
    "    \"L'intelligence artificielle transforme de nombreux secteurs économiques.\",\n",
    "    \"Je lis souvent des articles de recherche en finance et machine learning.\"\n",
    "]\n",
    "\n",
    "### ne conserver que 1000 mots dans le corpus :\n",
    "mon_tokenizer = Tokenizer(num_words=1000) \n",
    "mon_tokenizer.fit_on_texts(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDRUwZ86tkX8"
   },
   "source": [
    "> Quel est l'index du mot \"machine\" dans cet encodage ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'de',\n",
       " 2: 'je',\n",
       " 3: 'des',\n",
       " 4: 'et',\n",
       " 5: 'en',\n",
       " 6: 'les',\n",
       " 7: 'un',\n",
       " 8: 'la',\n",
       " 9: 'finance',\n",
       " 10: 'suis',\n",
       " 11: 'à',\n",
       " 12: 'machine',\n",
       " 13: 'learning',\n",
       " 14: 'sont',\n",
       " 15: 'dans',\n",
       " 16: 'est',\n",
       " 17: 'mes',\n",
       " 18: 'marchés',\n",
       " 19: 'étudiant',\n",
       " 20: 'ingénierie',\n",
       " 21: 'nantes',\n",
       " 22: \"j'adore\",\n",
       " 23: 'mathématiques',\n",
       " 24: 'appliquées',\n",
       " 25: 'modèles',\n",
       " 26: 'passionnants',\n",
       " 27: 'mixe',\n",
       " 28: 'régulièrement',\n",
       " 29: 'clubs',\n",
       " 30: 'festivals',\n",
       " 31: 'programmation',\n",
       " 32: 'python',\n",
       " 33: 'essentielle',\n",
       " 34: 'pour',\n",
       " 35: 'projets',\n",
       " 36: 'produis',\n",
       " 37: 'ma',\n",
       " 38: 'propre',\n",
       " 39: 'musique',\n",
       " 40: 'depuis',\n",
       " 41: 'plusieurs',\n",
       " 42: 'années',\n",
       " 43: 'financiers',\n",
       " 44: 'domaine',\n",
       " 45: 'complexe',\n",
       " 46: 'mais',\n",
       " 47: 'fascinant',\n",
       " 48: \"j'ai\",\n",
       " 49: 'travaillé',\n",
       " 50: 'sur',\n",
       " 51: 'projet',\n",
       " 52: 'prédiction',\n",
       " 53: 'électriques',\n",
       " 54: \"l'importance\",\n",
       " 55: 'variables',\n",
       " 56: 'forêts',\n",
       " 57: 'aléatoires',\n",
       " 58: \"m'intéresse\",\n",
       " 59: 'beaucoup',\n",
       " 60: 'passionné',\n",
       " 61: 'par',\n",
       " 62: \"l'analyse\",\n",
       " 63: 'données',\n",
       " 64: 'algorithmes',\n",
       " 65: 'participe',\n",
       " 66: 'compétitions',\n",
       " 67: 'data',\n",
       " 68: 'science',\n",
       " 69: 'mon',\n",
       " 70: 'objectif',\n",
       " 71: 'poursuivre',\n",
       " 72: 'master',\n",
       " 73: 'quantitative',\n",
       " 74: \"j'organise\",\n",
       " 75: 'événements',\n",
       " 76: 'gère',\n",
       " 77: 'équipes',\n",
       " 78: 'parallèle',\n",
       " 79: 'études',\n",
       " 80: 'joue',\n",
       " 81: 'du',\n",
       " 82: 'piano',\n",
       " 83: 'guitare',\n",
       " 84: 'batterie',\n",
       " 85: \"l'intelligence\",\n",
       " 86: 'artificielle',\n",
       " 87: 'transforme',\n",
       " 88: 'nombreux',\n",
       " 89: 'secteurs',\n",
       " 90: 'économiques',\n",
       " 91: 'lis',\n",
       " 92: 'souvent',\n",
       " 93: 'articles',\n",
       " 94: 'recherche'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "S9UuWgdRtkX9",
    "outputId": "0bf380e3-8d7c-46cb-8ae1-bb187e26b45e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "for key, value in mon_tokenizer.index_word.items():\n",
    "    if value == 'machine':\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNLZI-mFtkYE"
   },
   "source": [
    "> Afficher la liste des termes de ponctuations qui sont retirés par le Tokenizer. Modifier le filtre pour ne pas retirer le point d'exclamation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "4n2xQJJttkYH",
    "outputId": "075c7a92-8928-4f87-f4c9-6a5bf8acfec4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_tokenizer.filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_tokenizer.filters = '\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35kXP_2itkYP"
   },
   "source": [
    "> Transformer maintenant les mots en listes d'entiers avec la méthode `texts_to_sequences()` de la classe Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "E6StTuVStkYQ",
    "outputId": "e75aeb58-59cb-452c-d897-abf81efd7d56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 10, 7, 19, 5, 20, 11, 21],\n",
       " [22, 6, 23, 24, 4, 8, 9],\n",
       " [6, 25, 1, 12, 13, 14, 26],\n",
       " [2, 27, 28, 15, 3, 29, 4, 30],\n",
       " [8, 31, 5, 32, 16, 33, 34, 17, 35],\n",
       " [2, 36, 37, 38, 39, 40, 41, 42],\n",
       " [6, 18, 43, 14, 7, 44, 45, 46, 47],\n",
       " [48, 49, 50, 7, 51, 1, 52, 3, 18, 53],\n",
       " [54, 3, 55, 15, 6, 56, 57, 58, 59],\n",
       " [2, 10, 60, 61, 62, 1, 63, 4, 6, 64],\n",
       " [2, 65, 11, 3, 66, 1, 67, 68],\n",
       " [69, 70, 16, 1, 71, 7, 72, 5, 9, 73],\n",
       " [74, 3, 75, 4, 76, 3, 77, 5, 78, 1, 17, 79],\n",
       " [2, 80, 81, 82, 1, 8, 83, 4, 1, 8, 84],\n",
       " [85, 86, 87, 1, 88, 89, 90],\n",
       " [2, 91, 92, 3, 93, 1, 94, 5, 9, 4, 12, 13]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = mon_tokenizer.texts_to_sequences(samples)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHFrtLqBtkYX"
   },
   "source": [
    "# Word Embeddings (plongement des mots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7rcQuoztkYY"
   },
   "source": [
    "Il existe deux façons d'obtenir des embeddings de mots:\n",
    "\n",
    "- On peut apprendre un plongement pour une tache bien précise en amont (comme la classification des documents ou la prédiction des sentiments). Dans ce cas, on apprend le plongement comme on le fait pour un réseau de neurone classique.\n",
    "\n",
    "- On peut utiliser un embedding qui a été pré-entrainé pour une autre tâche, et que l'on \"recycle\" ici pour représenter les mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FDM9jQXtkYZ"
   },
   "source": [
    "### Apprentissage du plongement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpFEBhphtkYc"
   },
   "source": [
    "> En consultant la documentation sur la couche [`Embedding` de Keras](https://keras.io/layers/embeddings/), indiquer quels paramètres faut-il donner en argument à la fonction `Embedding` pour que celle-ci puisse représenter un plongement dans un espace de dimension 64 de séquences de longeur 10 mots dans corpus de 1000 mots retenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "DiMZrn9GtkYd",
    "outputId": "1f4aa5d7-43bd-4467-8089-cc7a9d4bc10d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(1000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXrINO1CtkYh"
   },
   "source": [
    "La couche `Embedding` prend en entrée un tenseur 2D d'entiers, de taille  `nombre de séquences` x  `longueur d'une séquence`.\n",
    "\n",
    "Toutes les séquences dans un bacth (de séquences) doivent donc avoir la même longueur, quitte à tronquer ou compléter avec des zeros les séquences trop longues ou trop courtes.\n",
    "\n",
    "Cette couche renvoie un tenseur 3D de valeurs numériques de taille `nombre de séquences` x  `longueur d'une séquence` x `dim d'arrivée du plongement`. \n",
    "\n",
    "Ces tenseurs 3D  peuvent ensuite être connectés à des couches récurrentes ou convolutionnelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgKde-WgtkYh"
   },
   "source": [
    "Dans un réseau de neurones, nous allons maintenant créer une première couche de plongement (embedding layer) et nous allons apprendre les poids de ce plongement exactement comme on le fait pour une couche dense.  Nous allons pour cela utiliser les données [imdb newswires Reuters](https://keras.io/datasets/#reuters-newswire-topics-classification) qui peuvent être directement chargées dans keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yKRnp7gMtkYi",
    "outputId": "4830d804-7b61-4633-dad5-3653c86d4970"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "max_mots = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_mots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XSpeleYtkYl"
   },
   "source": [
    "L'argument `num_words` correspond au nombre maximal de mots utilisés comme features. On le limite ici à 10000.\n",
    "\n",
    "> Vérifier que les mots ont été chargés sous la forme d'entiers. Que représente ici y ? Quel est l'objectif de ce problème d'apprentissage ? On parle \"d'analyse de sentiment\" (sentiment analysis ou opinion mining). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L3LuqLF3tkYl",
    "outputId": "072c5639-0872-4b72-c80a-bd7a5ab4d860"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Bien chargés sous forme d'entiers\n",
    "print(x_train[0])\n",
    "# Classification Binaire (des 0 ou des 1)\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYmkYuVrtkYo"
   },
   "source": [
    "> Bonus : retrouver les phrases à partir des vecteurs d'entiers (voir la doc de `imdb.load_dat`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTPAayRttkYo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "word_index\n",
    "\n",
    "reverse_word_index = {i + 3: word for word, i in word_index.items()}\n",
    "reverse_word_index[0] = \"<PAD>\"\n",
    "reverse_word_index[1] = \"<START>\"\n",
    "reverse_word_index[2] = \"<UNK>\"\n",
    "reverse_word_index[3] = \"<UNUSED>\"\n",
    "\n",
    "def decode_review(encoded_review):\n",
    "    return \" \".join([reverse_word_index.get(i, \"?\") for i in encoded_review])\n",
    "\n",
    "# Exemple de décodage d'un avis\n",
    "print(decode_review(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0W8nqgotkYs"
   },
   "source": [
    "> Utiliser la fonction [preprocessing.sequence.pad_sequences](https://keras.io/api/preprocessing/timeseries/#padsequences-function) pour transformer `x_train` et `x_test` en deux tenseurs 2D de tailles `nb de sequences` x  `long max d une sequence = 20`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "s0S7qsz-tkYs",
    "outputId": "1f0bd33d-cf99-4824-eda7-aa7ec1ec25e4"
   },
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "\n",
    "x_train_normal = pad_sequences(x_train, maxlen=20, truncating='post')\n",
    "x_test_normal = pad_sequences(x_test, maxlen=20, truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "raVGkng6tkYu"
   },
   "source": [
    "> Construire un réseau à propagation avant comme suit:\n",
    "- Une couche d'embedding qui plonge chaque mot dans un espace de dimension 8.\n",
    "- Une couche Flatten pour redimensionner le tenseur 3D des plongements en un tenseur 2D  de taille `nb de sequences` x  (8*20)  \n",
    "- Une couche dense avec activation sigmoid pour la classification finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "0Br819HBtkYv",
    "outputId": "50541480-1a21-4506-8bd1-e76adb38c9e8"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Flatten, Dense, Activation, InputLayer\n",
    "Couche_Embedding = Embedding(input_dim = 10000, output_dim = 8)\n",
    "Couche_Flatten = Flatten()\n",
    "Couche_Dense = Dense(1)\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "model = Sequential()\n",
    "model.add(InputLayer(shape=(20, )))\n",
    "model.add(Couche_Embedding)\n",
    "model.add(Couche_Flatten)\n",
    "model.add(Couche_Dense)\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-RejtiAtkYx"
   },
   "source": [
    "> Utiliser un optimiseur `rmsprop` avec perte `binary_crossentropy` et suivi de la métrique `acc` (précision) le long de la trajectoire d'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "GZV3J0HHtkYy",
    "outputId": "6187d5de-f5c0-4b3a-ac34-059823397a45"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.5)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1C5JLdxtkY0"
   },
   "source": [
    "> Affichez le résumé du réseau de neurones ainsi construit et assurez-vous que vous comprenez les dimensions affichées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "h7jle7NftkY0",
    "outputId": "ce7d01d1-707b-440b-d746-f71faab4d5bb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_22\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_22\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">80,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_25 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m8\u001b[0m)          │        \u001b[38;5;34m80,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_24 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m160\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m161\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_15 (\u001b[38;5;33mActivation\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,161</span> (313.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,161\u001b[0m (313.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,161</span> (313.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m80,161\u001b[0m (313.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3bzGP3AtkY2"
   },
   "source": [
    "> Ajuster le modèle sur les données d'apprentissage et donner la précision de validation finale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "GABhvI-AtkY2",
    "outputId": "0a13e1a6-91c7-47e3-9109-a9ac944b53ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 506us/step - acc: 0.5398 - loss: 13.1405\n",
      "Epoch 2/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 512us/step - acc: 0.6228 - loss: 17.0788\n",
      "Epoch 3/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - acc: 0.6511 - loss: 18.5644\n",
      "Epoch 4/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - acc: 0.6639 - loss: 18.9385\n",
      "Epoch 5/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 553us/step - acc: 0.6940 - loss: 18.9199\n",
      "Epoch 6/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 505us/step - acc: 0.6983 - loss: 19.3424\n",
      "Epoch 7/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - acc: 0.7205 - loss: 19.2394\n",
      "Epoch 8/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - acc: 0.7327 - loss: 19.0382\n",
      "Epoch 9/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 555us/step - acc: 0.7495 - loss: 19.0151\n",
      "Epoch 10/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - acc: 0.7542 - loss: 18.7461\n",
      "Epoch 11/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - acc: 0.7731 - loss: 18.5361\n",
      "Epoch 12/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - acc: 0.7880 - loss: 18.0158\n",
      "Epoch 13/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - acc: 0.8008 - loss: 17.7252\n",
      "Epoch 14/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - acc: 0.8179 - loss: 17.2780\n",
      "Epoch 15/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 500us/step - acc: 0.8270 - loss: 17.1205\n",
      "Epoch 16/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - acc: 0.8419 - loss: 16.0276\n",
      "Epoch 17/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - acc: 0.8505 - loss: 16.1203\n",
      "Epoch 18/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - acc: 0.8627 - loss: 15.3592\n",
      "Epoch 19/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - acc: 0.8745 - loss: 14.8079\n",
      "Epoch 20/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step - acc: 0.8814 - loss: 14.7175\n",
      "Epoch 21/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 552us/step - acc: 0.8889 - loss: 14.7348\n",
      "Epoch 22/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - acc: 0.8989 - loss: 14.4107\n",
      "Epoch 23/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - acc: 0.8975 - loss: 13.7777\n",
      "Epoch 24/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - acc: 0.9069 - loss: 13.6136\n",
      "Epoch 25/25\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - acc: 0.9146 - loss: 13.2868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x289dd1790>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_normal, y_train, epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nJhiFCHYtkY5"
   },
   "source": [
    "Le taux de bien classés tourne autour de 75%, ce qui est correct, mais on peut espérer faire mieux en utilisant le caractère \"séquentiel\" des phrases, grâce à des réseaux récurrents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlRRtQBftkY5"
   },
   "source": [
    "# Construction d'un réseau récurrent simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "339UmcyytkY6"
   },
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9m0qT6d4tkY7"
   },
   "source": [
    "Une couche `SimpleRNN` prend en entrée un tenseur 3D de taille `batch_size` x `timesteps` (longeur de la séquence) x  `input_features` (typiquement la dimension de l'embedding). \n",
    "\n",
    "Comme tous les modèles récurrents, `SimpleRNN` peut renvoyer la suite complète de toutes les sorties pour chaque temps (le long de la séquence), ou bien tout simplement la denière sortie pour chaque séquence. \n",
    "\n",
    "> Expliquer la différence de dimension observée sur la couche récurrente dans les deux architectures proposées ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "zxzlwjjMtkY8",
    "outputId": "21751960-988e-4f4a-a686-7672e6466ea4"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = 1000,output_dim=32,input_length = 10))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "4e9H80STtkY9",
    "outputId": "c02abeeb-81cb-4871-bf88-38ae0f4407c0"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = 1000,output_dim=32,input_length = 10))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URasM4qDtkY_"
   },
   "source": [
    "> Préparer des données lexicales d'apprentissage et de test pour les données [`imdb`](https://keras.io/api/datasets/imdb/#load_data-function) selon les spécifications suivantes:\n",
    "- nombre de mots pris en compte : 10000 \n",
    "- longeur maximale des séquences : 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "L8R_jhZAtkY_",
    "outputId": "15be8d17-b515-4f86-b34b-f8a2651ca1f8"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwjsjJgdtkZB"
   },
   "source": [
    "> Construire un réseau à propagation avant comme suit:\n",
    "- Une couche d'embedding qui plonge chaque mot dans un espace de dimension 32.\n",
    "- Une couche `SimpleRNN` avec uniquement sortie finale\n",
    "- Une couche dense avec activation sigmoid pour la classification finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xV0RW75ttkZC"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNn_c4LMtkZD"
   },
   "source": [
    "> Utiliser un optimiseur `rmsprop` avec perte `binary_crossentropy` et suivi de la métrique `acc` (précision) le long de la trajectoire d'optimisation. Ajuster le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cbUbqektkZE"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5W0vrlNGwBgg"
   },
   "source": [
    "# Construction d'un réseau récurrent avec cellules LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_dK3HRtwhak"
   },
   "source": [
    "> Construire enfin un réseau similaire où vous aurez remplacé la couche SimpleRNN par une couche [LSTM](https://keras.io/layers/recurrent/#lstm).\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "6nSBWWW2wgv1",
    "outputId": "e7d7eeb6-09d9-4d17-fc4c-315807b9dffe"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06AUej2ttkZU"
   },
   "source": [
    "# Bonus : utilisation d'un embedding pré-entrainé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DI1PFMdtkZV"
   },
   "source": [
    "Cette fois nous allons partir des données `Imdb` brutes et plonger celles-ci dans un espace via un plongement qui a déjà été ajusté (sur des données différentes et pour un problème autre). \n",
    "\n",
    "Nous allons utiliser [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove) : télécharger glove.6B.zip (près d'un giga !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Wa3SLdDtkZV"
   },
   "source": [
    "> Télécharger les données brutes à [cette adresse](http://mng.bz/0tIo). Les textes positifs et négatifs sont classés dans des repertoires de même nom. Compléter le code ci-dessous pour importer et préparer les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tmklq8VptkZV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = ### TO DO ###\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in [### TO DO ###]:\n",
    "    dir_name = os.path.join(train_rep, ### TO DO ###)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, ### TO DO ###))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == ### TO DO ###:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJFHlNCItkZX",
    "outputId": "59fa292a-4360-490d-95c0-773eb3d9b566"
   },
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Px8rgO_TtkZa"
   },
   "source": [
    "> Effectuer les opérations de traitement lexical (tokenization) pour un corpus de 10000 mots et des séquence de mots d'une longueur maximale de 100 mots. Transformer `labels` en un vecteur `numpy`. Vérifier les dimensions des objets construits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rZyddzitkZt",
    "outputId": "2b8cc3f2-6a54-438b-f407-4d055b24f9e3"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lM1r5wmctkZw"
   },
   "source": [
    "> Extraire 1000 données pour l'apprentissage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FH37Eu7tkZw"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WeltkAkhtkZx"
   },
   "source": [
    "> En suivant la documentation de Keras sur cette [page](https://keras.io/examples/nlp/pretrained_word_embeddings/), utiliser  un embedding de type Glove sur les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUdFEwuStkZy"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwVzJ3NptkZ0"
   },
   "source": [
    "> Utiliser cet embedding pour construire des réseaux recurrents ou non pour prédire la sortie Y. Evaluer la précision de vos modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8lsAZQVtkZ0"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDRxcABAtkZ5"
   },
   "source": [
    "> Utiliser cet embedding de mots pour évaluer la proximité entre quelques phrases que vous choisirez. Vous pourrez représenter les données dans le premier plan factoriel d'une ACP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IRzZUbSptkZ6"
   },
   "outputs": [],
   "source": [
    "### TO DO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulter cette [page](https://keras.io/examples/nlp/text_classification_with_transformer/) pour une illustration de l'utilisation des transformers sur les données IMBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adpater le code de cette [page](https://keras.io/examples/nlp/text_classification_with_transformer/) pour traiter les données imdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN-NLP_correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
